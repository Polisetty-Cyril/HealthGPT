{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 1: FIX DEPENDENCIES - RUN FIRST AFTER RESTART\n# ============================================================================\n\nimport subprocess\nimport sys\n\nprint('🔧 Fixing package dependencies...')\n\n# Fix PyArrow compatibility - install version 15.0.2 for bigframes\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], \n                capture_output=True, check=False)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow==15.0.2\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Fix rich version for bigframes compatibility\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rich==13.7.1\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Install google-cloud-bigquery-storage (missing dependency)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-bigquery-storage>=2.30.0\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Upgrade google-cloud-bigquery for bigframes\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"google-cloud-bigquery>=3.31.0\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Upgrade google-api-core for pandas-gbq\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"google-api-core>=2.10.2\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Install missing packages including faiss-cpu and fix protobuf\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"keybert\", \"rank-bm25\", \"evaluate\", \"faiss-cpu\", \"protobuf<5.0.0\"], \n                check=True)\n\nprint('✅ Dependencies fixed and installed')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T17:02:30.793709Z","iopub.execute_input":"2025-10-26T17:02:30.793952Z","iopub.status.idle":"2025-10-26T17:04:37.572320Z","shell.execute_reply.started":"2025-10-26T17:02:30.793927Z","shell.execute_reply":"2025-10-26T17:04:37.571613Z"}},"outputs":[{"name":"stdout","text":"🔧 Fixing package dependencies...\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 163.4 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 15.0.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 12.8 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.6/293.6 kB 13.6 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 259.3/259.3 kB 16.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.1/167.1 kB 313.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 214.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 323.2/323.2 kB 335.0 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.27.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 2.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 5.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.4/31.4 MB 61.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 19.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.7/47.7 MB 37.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 86.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 68.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 48.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 2.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 25.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 12.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 8.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 69.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 566.1/566.1 kB 34.2 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.76.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.27.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"✅ Dependencies fixed and installed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: VERIFY ALL IMPORTS - RUN SECOND\n# ============================================================================\n!pip install sacremoses\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"🔍 Testing imports...\")\n\ntry:\n    from datasets import load_dataset\n    print(\"✅ datasets\")\n    from sentence_transformers import SentenceTransformer\n    print(\"✅ sentence-transformers\")\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    print(\"✅ transformers\")\n    import faiss\n    print(\"✅ faiss\")\n    from keybert import KeyBERT\n    print(\"✅ keybert\")\n    from rank_bm25 import BM25Okapi\n    print(\"✅ rank-bm25\")\n    import torch\n    print(f\"✅ torch (device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n    print(\"\\n🎉 ALL IMPORTS SUCCESSFUL!\")\nexcept Exception as e:\n    print(f\"❌ Import failed: {e}\\nPlease restart kernel and try again.\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T17:06:08.101700Z","iopub.execute_input":"2025-10-26T17:06:08.102370Z","iopub.status.idle":"2025-10-26T17:06:38.289267Z","shell.execute_reply.started":"2025-10-26T17:06:08.102343Z","shell.execute_reply":"2025-10-26T17:06:38.288392Z"}},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2025.9.18)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n🔍 Testing imports...\n✅ datasets\n","output_type":"stream"},{"name":"stderr","text":"2025-10-26 17:06:24.885388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761498385.057304      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761498385.108458      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✅ sentence-transformers\n✅ transformers\n✅ faiss\n✅ keybert\n✅ rank-bm25\n✅ torch (device: cuda)\n\n🎉 ALL IMPORTS SUCCESSFUL!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"=== women-health sample ===\")\nds1 = load_dataset(\"altaidevorg/women-health-mini\", split=\"train\")\nfor i in range(2):\n    print(ds1[i])\n\nprint(\"\\n=== medical-qa sample ===\")\nds2 = load_dataset(\"Malikeh1375/medical-question-answering-datasets\", \"all-processed\", split=\"train\")\nfor i in range(2):\n    print(ds2[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T17:06:38.290641Z","iopub.execute_input":"2025-10-26T17:06:38.291149Z","iopub.status.idle":"2025-10-26T17:06:45.418407Z","shell.execute_reply.started":"2025-10-26T17:06:38.291129Z","shell.execute_reply":"2025-10-26T17:06:45.417785Z"}},"outputs":[{"name":"stdout","text":"=== women-health sample ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0026ed2a31b14cdcbfbfa93d871be4e8"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"women-health-mini.jsonl:   0%|          | 0.00/35.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4fa0feb84f84959acf4b5daa358e6df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10348 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3a05c1fd874e38826e5e559c5c258f"}},"metadata":{}},{"name":"stdout","text":"{'conversations': [{'role': 'user', 'content': \"I had an unsafe abortion and now I'm experiencing complications. Can you help me?\"}, {'role': 'assistant', 'content': \"I understand you're going through a very difficult time, and I want to assure you that I'm here to help.  Having an unsafe abortion can lead to serious complications, and it's crucial to address them immediately.  \\n\\nThe potential complications following an unsafe abortion are significant and can range from mild to life-threatening. These can include:\\n\\n* **Incomplete abortion:** This means some pregnancy tissue remains in the uterus, leading to continued bleeding, pain, and infection.\\n* **Infection:**  This is a major concern and can manifest as fever, chills, severe abdominal pain, foul-smelling vaginal discharge, and weakness.  Untreated infection can spread to the bloodstream (septicemia), leading to severe illness and even death.\\n* **Excessive bleeding (hemorrhage):** This is a life-threatening emergency and can cause shock and potentially death.\\n* **Damage to the cervix or uterus:** This can lead to future complications with fertility or pregnancy.\\n* **Perforation of the uterus:**  This is a serious complication where the uterus is punctured, requiring immediate surgical intervention.\\n\\nGiven the seriousness of the potential complications, it's absolutely vital that you seek immediate medical attention.  Do not delay.  The sooner you receive appropriate medical care, the better the outcome.  A healthcare professional can perform a thorough examination, determine the extent of any complications, and provide the necessary treatment, which may include:\\n\\n* **Medication:**  To manage infection, pain, and bleeding.\\n* **Surgical intervention:**  To remove any remaining pregnancy tissue (D&C or manual vacuum aspiration) or repair any damage.\\n* **Blood transfusion:** If you have experienced significant blood loss.\\n* **Monitoring:** To ensure your recovery and address any potential long-term complications.\\n\\n\\nI understand you may feel apprehensive about seeking medical help, but please know that healthcare professionals are obligated to provide you with confidential and non-judgmental care.  Your privacy will be respected.  Focus on your health and safety – that's the most important thing right now.  Please go to your nearest hospital emergency room or a trusted healthcare provider immediately.  If you have any difficulty accessing care, I encourage you to reach out to organizations that can provide support and guidance in navigating the healthcare system.\\n\\n\\nRemember, you are not alone, and seeking help is a brave and essential step towards your recovery.  Your well-being is my priority. Please get immediate medical help.\\n\"}]}\n{'conversations': [{'role': 'user', 'content': 'Are there any particular challenges faced by visually impaired women during menstruation?'}, {'role': 'assistant', 'content': \"Visually impaired women face unique challenges during menstruation, primarily revolving around accessibility and independence.  Let's break down some key areas:\\n\\n* **Product Management:**  Identifying and managing menstrual products can be difficult.  Differentiating between different absorbencies, sizes, and types of tampons, pads, or menstrual cups solely through touch can be challenging.  This necessitates a well-organized system, perhaps with products clearly labeled via braille or tactile markings (if available).  It's also crucial to have a reliable system for storing used products hygienically until disposal.\\n\\n* **Tracking the Menstrual Cycle:**  Traditional calendar methods may be difficult without the ability to easily read dates.  However, many menstrual tracking apps are available with voice-activated interfaces or large-font options that can greatly assist in monitoring cycle length and flow.  A supportive friend or family member can also help with tracking if necessary.\\n\\n* **Hygiene and Application:**  Applying tampons or menstrual cups requires dexterity and spatial awareness. While some practice and adaptive techniques can improve proficiency, assistance may be required initially, especially with menstrual cups.  Thorough handwashing, crucial for menstrual hygiene, is facilitated through tactile cues and auditory confirmation of water running.\\n\\n* **Disposal:**  Safe and hygienic disposal of menstrual products requires clear spatial awareness, something that visual impairment can impact.  Implementing a well-defined and accessible disposal system within the bathroom can mitigate this challenge.\\n\\n* **Seeking Medical Assistance:**  Describing symptoms and concerns to medical professionals can be challenging, particularly if the woman is unable to easily show physical symptoms. Clear and detailed verbal communication is critical, supplemented by any other available means such as tactile demonstrations if appropriate and the healthcare professional is comfortable with this.\\n\\n* **Emotional and Psychological Well-being:** The added stress of navigating menstruation with a visual impairment can be significant.  Access to support networks, both visually impaired-specific and general women's health support groups, can greatly improve emotional well-being.\\n\\n\\nIt's vital to emphasize that these challenges are surmountable. With appropriate planning, assistive technologies, and support systems, visually impaired women can manage their periods effectively and maintain excellent menstrual health.  Don't hesitate to experiment with different products and methods to find what works best for you, and remember that support is available.  Consider contacting organizations specializing in assistive technology and disability services for tailored guidance and resources.  Your comfort and wellbeing are paramount.\\n\"}]}\n\n=== medical-qa sample ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39232b2e52d741c6857a57f27cefa767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all-processed/train-00000-of-00001-9bfe4(…):   0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bbb83018c18482faa2c559513760d67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/246678 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03f4a5cb7b2e45e1988ea3ac57ac4b2b"}},"metadata":{}},{"name":"stdout","text":"{'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\", 'input': 'Hey Just wondering.  I am a 39 year old female, pretty smallMy heart rate is around 97 to 106 at rest, and my BP is 140/90 and twice I get 175/118I did visit a doctor because I  didnt feel well past month or twoThen the doctor gave me a heart medicine to take the pulse down and BP  (its still in further examination.)But I wondering what it can be? Do I need the medicine really?  Is that bad ?', 'output': \"hello and thank you for using chatbot. i carefully read your question and i understand your concern. i will try to explain you something and give you my opinion. we talk about hypertension if we have mean value that exceeds 140 / 90 mmhg. a person might have high value during emotional and physicals trees so it's mandatory to judge on mean values. usaly hypertension does not give any symptoms but left untreated he slowly modifies the heart. according to heart rhythm, the normal rate is between 50-100 beat for minute. when it exceeds 100 we talk about sinus tachycardia. this might have different causes to simple emotional stress, physical activity, coffee consumption or pathologies like anemia, hyperthyroidism. so if we diagnose hypertension and rhythm issue we have to find they cause and of course treat them. if you treat the hypertension than you have nothing to worry. if i was your treating doctor i will recommend some examination like an electrocardiogram, a cardiac echo, a full blood analyze, a holder rhythm and pressure monitoring. this gives a better view how to treat the problem, medical or not. but as you catch values up to 170 i think medical treatment is necessary. hope i was helpful. wish you good health. best regards.\", '__index_level_0__': 157271}\n{'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\", 'input': 'I had a brain anyurism in 1993. I used to have severe migrains then after the anyurism I had no more.I am now experiencing some shooting pains off and on in my head and severe memory problems I forget what Im trying to save and that happens daily .Had a spell the other day and I couldn t speak at all. Do these symtoms tell of maybe another anyurism or brain tumor that. I appricaie any help ty.', 'output': 'hello brain tumor may present in various ways. it may present as headache, seizure(generalized or focal) etc. you may need detail neurological examination and investigations. investigation include ct scan of brain, eeg etc. mri can be done if needed. contrast ct scan or mri should be done as you have history of brain aneurysm. i advise consulting neurologist for detail evaluation. get well soon. take care chatbot.', '__index_level_0__': 235698}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# MULTI-DOMAIN RAG PIPELINE FOR MEDICAL QA\n# Complete production-ready implementation\n# Datasets: Women's Health + General Medical QA\n# Models: all-MiniLM (embedder), BGE-reranker, BioGPT (HyDE), Flan-T5 (generator)\n# ============================================================================\n\nimport os, sys, time, json, pickle, re, warnings, random\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport torch\nfrom pathlib import Path\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional, Any, Tuple\nimport logging\nfrom datetime import datetime\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nfrom keybert import KeyBERT\nimport faiss\nfrom rank_bm25 import BM25Okapi\n\n# ============================================================================\n# REPRODUCIBILITY & SETUP\n# ============================================================================\n\ndef set_all_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_all_seeds(42)\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Download NLTK data\nfor resource in ['punkt', 'stopwords']:\n    try:\n        nltk.data.find(f'tokenizers/{resource}')\n    except LookupError:\n        nltk.download(resource, quiet=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"🚀 Device: {device}\")\nif torch.cuda.is_available():\n    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}, Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n@dataclass\nclass DomainConfig:\n    name: str\n    dataset_name: str\n    config_name: str = None  # ← ADD THIS LINE\n    dataset_split: str = \"train\"\n    index_path: str = None\n    id2doc_path: str = None\n    metadata_path: str = None\n\n    \n    def __post_init__(self):\n        if self.index_path is None:\n            self.index_path = f\"{self.name}_faiss.index\"\n        if self.id2doc_path is None:\n            self.id2doc_path = f\"{self.name}_id2doc.pkl\"\n        if self.metadata_path is None:\n            self.metadata_path = f\"{self.name}_metadata.json\"\n\n@dataclass\nclass RAGConfig:\n    embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    reranker_model: str = \"BAAI/bge-reranker-large\"\n    hyde_model: str = \"gpt2\"\n    generator_model: str = \"microsoft/BioGPT-Large\"\n    \n    chunk_window: int = 3\n    chunk_stride: int = 1\n    retrieve_k: int = 30\n    rerank_topk: int = 8\n    context_chunks: int = 4\n    hyde_weight: float = 0.4\n    faiss_alpha: float = 0.6\n    \n    max_new_tokens: int = 200\n    hyde_max_tokens: int = 60\n    \n    completeness_threshold: float = 0.65\n    faithfulness_threshold: float = 0.55\n    \n    retrieval_weight: float = 0.4\n    completeness_weight: float = 0.3\n    faithfulness_weight: float = 0.3\n    \n    prompts_log: str = \"prompts_outputs.pkl\"\n    random_seed: int = 42\n    test_size: float = 0.15\n\nDOMAINS = [\n    DomainConfig(name=\"women_health\", dataset_name=\"altaidevorg/women-health-mini\"),\n    DomainConfig(name=\"medical_qa\", dataset_name=\"Malikeh1375/medical-question-answering-datasets\", config_name=\"all-processed\")\n]\n\nconfig = RAGConfig()\n\n# ============================================================================\n# UTILITIES\n# ============================================================================\n\ndef clean_text_artifacts(text: str) -> str:\n    text = re.sub(r\"^(Answer:|Final answer:|Response:)\\s*\", \"\", text, flags=re.IGNORECASE)\n    text = re.sub(r\"<\\/?[^>]+>|</s>|▃|\\[INST\\]|\\[/INST\\]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text.strip(\" \\n\\r\\t\\\"'\")\n\ndef monitor_memory():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        logger.info(f\"💾 GPU: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n        if allocated/total > 0.85:\n            torch.cuda.empty_cache()\n\n# ============================================================================\n# DATA LOADING\n# ============================================================================\n\n@staticmethod\ndef extract_qa_pairs(dataset, domain_name: str) -> List[Dict[str, Any]]:\n    qa_data = []\n    for idx, row in enumerate(dataset):\n        try:\n            # For women-health-mini (conversation-based)\n            if 'conversations' in row and isinstance(row['conversations'], list):\n                conversations = row['conversations']\n                question = \"\"\n                answer = \"\"\n                for msg in conversations:\n                    if msg.get(\"role\") == \"user\" and not question:\n                        question = msg.get(\"content\", \"\")\n                    if msg.get(\"role\") == \"assistant\" and not answer:\n                        answer = msg.get(\"content\", \"\")\n                if question and answer:\n                    qa_data.append({\n                        \"question\": question.strip(),\n                        \"answer\": answer.strip(),\n                        \"domain\": domain_name,\n                        \"source_id\": idx\n                    })\n                    continue\n            # For medical_qa formats\n            if 'question' in row and 'answer' in row:\n                qa_data.append({\n                    \"question\": str(row['question']).strip(),\n                    \"answer\": str(row['answer']).strip(),\n                    \"domain\": domain_name,\n                    \"source_id\": idx\n                })\n                continue\n            if 'input' in row and 'output' in row:\n                qa_data.append({\n                    \"question\": str(row['input']).strip(),\n                    \"answer\": str(row['output']).strip(),\n                    \"domain\": domain_name,\n                    \"source_id\": idx\n                })\n        except Exception as e:\n            if idx < 2:\n                print(f\"extract_qa_pairs WARNING, row {idx}: {e}\")\n            continue\n    return qa_data\n\n\n    @staticmethod\n    def load_domain_data(domain_config: DomainConfig) -> Tuple[List[Dict], List[Dict]]:\n        logger.info(f\"📥 Loading {domain_config.name}...\")\n        try:\n            # The dataset loading logic:\n            if domain_config.config_name:\n                dataset = load_dataset(domain_config.dataset_name, domain_config.config_name, split=domain_config.dataset_split)\n            else:\n                dataset = load_dataset(domain_config.dataset_name, split=domain_config.dataset_split)\n            logger.info(f\"Dataset {domain_config.name} loaded with {len(dataset)} rows\")\n            qa_data = DatasetLoader.extract_qa_pairs(dataset, domain_config.name)\n            if not qa_data:\n                logger.error(f\"No QA pairs extracted from {domain_config.name}\")\n                logger.error(f\"Sample row structure: {dataset[0]}\")\n                raise ValueError(f\"No QA pairs extracted from {domain_config.name}. Check dataset structure.\")\n            train_data, test_data = train_test_split(\n                qa_data, test_size=config.test_size, random_state=config.random_seed\n            )\n            logger.info(f\"✅ {domain_config.name}: {len(train_data)} train, {len(test_data)} test\")\n            return train_data, test_data\n        except Exception as e:\n            logger.error(f\"❌ Failed to load {domain_config.name}: {e}\")\n            raise\n\n\n\n# ============================================================================\n# TEXT CHUNKING\n# ============================================================================\n\nclass TextChunker:\n    @staticmethod\n    def create_chunks(data: List[Dict], window: int = 3, stride: int = 1, min_chars: int = 50) -> List[Dict]:\n        chunks = []\n        for item in data:\n            text = item.get(\"answer\", \"\")\n            if not text or len(text) < min_chars:\n                continue\n            \n            sentences = sent_tokenize(text)\n            if not sentences:\n                continue\n            \n            if len(sentences) <= window:\n                chunks.append({\n                    \"chunk\": \" \".join(sentences),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks)\n                })\n                continue\n            \n            for i in range(0, max(1, len(sentences) - window + 1), stride):\n                chunks.append({\n                    \"chunk\": \" \".join(sentences[i:i + window]),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks),\n                    \"window\": (i, i + window)\n                })\n        return chunks\n\n# ============================================================================\n# MODEL MANAGEMENT\n# ============================================================================\n\nclass ModelManager:\n    def __init__(self, config: RAGConfig, device: torch.device):\n        self.config = config\n        self.device = device\n        self.models = {}\n    \n    def load_embedder(self):\n        logger.info(f\"📦 Loading embedder...\")\n        embedder = SentenceTransformer(self.config.embed_model, device=self.device)\n        self.models['embedder'] = embedder\n        logger.info(f\"✅ Embedder loaded\")\n        return embedder\n    \n    def load_reranker(self):\n        logger.info(f\"📦 Loading reranker...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.reranker_model)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            self.config.reranker_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        ).to(self.device)\n        model.eval()\n        self.models['reranker_tokenizer'] = tokenizer\n        self.models['reranker_model'] = model\n        logger.info(f\"✅ Reranker loaded\")\n        return tokenizer, model\n    \n    def load_hyde_model(self):\n        logger.info(f\"📦 Loading HyDE model...\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(self.config.hyde_model)\n            model = AutoModelForCausalLM.from_pretrained(\n                self.config.hyde_model,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                low_cpu_mem_usage=True\n            ).to(self.device)\n            model.eval()\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            self.models['hyde_tokenizer'] = tokenizer\n            self.models['hyde_model'] = model\n            logger.info(f\"✅ HyDE model loaded\")\n            return tokenizer, model\n        except Exception as e:\n            logger.warning(f\"⚠️ HyDE load failed, using query expansion: {e}\")\n            return None, None\n    \n    def load_generator(self):\n        logger.info(f\"📦 Loading generator...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.generator_model)\n        model = AutoModelForCausalLM.from_pretrained(\n            self.config.generator_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            low_cpu_mem_usage=True\n        ).to(self.device)\n        model.eval()\n        self.models['gen_tokenizer'] = tokenizer\n        self.models['gen_model'] = model\n        logger.info(f\"✅ Generator loaded\")\n        return tokenizer, model\n    \n    def load_keyword_extractor(self):\n        try:\n            kw_model = KeyBERT(model=self.models.get('embedder'))\n            self.models['keyword_extractor'] = kw_model\n            logger.info(f\"✅ KeyBERT loaded\")\n            return kw_model\n        except Exception as e:\n            logger.warning(f\"⚠️ KeyBERT load failed: {e}\")\n            return None\n    \n    def load_all(self):\n        logger.info(\"🔧 Loading all models...\")\n        self.load_embedder()\n        self.load_reranker()\n        self.load_hyde_model()\n        self.load_generator()\n        self.load_keyword_extractor()\n        monitor_memory()\n        logger.info(\"✅ All models loaded\")\n        return self.models\n\n# ============================================================================\n# INDEX MANAGEMENT\n# ============================================================================\n\nclass MultiDomainIndexManager:\n    def __init__(self, config: RAGConfig, embedder: SentenceTransformer):\n        self.config = config\n        self.embedder = embedder\n        self.domain_indices = {}\n    \n    def build_or_load_domain_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        if Path(domain_config.index_path).exists() and Path(domain_config.id2doc_path).exists():\n            try:\n                return self._load_existing_index(domain_config)\n            except:\n                pass\n        return self._build_new_index(domain_config, chunks)\n    \n    def _load_existing_index(self, domain_config: DomainConfig) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"📂 Loading existing {domain_config.name} index...\")\n        index = faiss.read_index(domain_config.index_path)\n        with open(domain_config.id2doc_path, \"rb\") as f:\n            id2doc = pickle.load(f)\n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        logger.info(f\"✅ Loaded {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def _build_new_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"🔨 Building {domain_config.name} index...\")\n        id2doc = [chunk[\"chunk\"] for chunk in chunks]\n        \n        embeddings = self.embedder.encode(\n            id2doc, normalize_embeddings=True, show_progress_bar=True,\n            batch_size=64, convert_to_numpy=True\n        ).astype('float32')\n        \n        dim = embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(embeddings)\n        \n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        \n        faiss.write_index(index, domain_config.index_path)\n        with open(domain_config.id2doc_path, \"wb\") as f:\n            pickle.dump(id2doc, f)\n        \n        metadata = {\"created_at\": time.time(), \"n_vectors\": int(index.ntotal), \"embedding_dim\": dim, \"domain\": domain_config.name}\n        with open(domain_config.metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        \n        logger.info(f\"✅ Built {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def load_all_domains(self, domain_chunks: Dict[str, List[Dict]]):\n        for domain in DOMAINS:\n            index, id2doc, bm25 = self.build_or_load_domain_index(domain, domain_chunks.get(domain.name, []))\n            self.domain_indices[domain.name] = {\n                'index': index, 'id2doc': id2doc, 'bm25': bm25, 'config': domain\n            }\n        logger.info(f\"✅ Loaded {len(self.domain_indices)} domain indices\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T17:06:45.419182Z","iopub.execute_input":"2025-10-26T17:06:45.419456Z","iopub.status.idle":"2025-10-26T17:06:45.863354Z","shell.execute_reply.started":"2025-10-26T17:06:45.419428Z","shell.execute_reply":"2025-10-26T17:06:45.862732Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# QUERY ROUTER\n# ============================================================================\n\nclass QueryRouter:\n    def __init__(self, embedder: SentenceTransformer, domain_indices: Dict):\n        self.embedder = embedder\n        self.domain_indices = domain_indices\n        self.domain_centroids = self._compute_centroids()\n    \n    def _compute_centroids(self) -> Dict[str, np.ndarray]:\n        centroids = {}\n        logger.info(\"🎯 Computing domain centroids...\")\n        for domain_name, domain_data in self.domain_indices.items():\n            id2doc = domain_data['id2doc']\n            sample_docs = random.sample(id2doc, min(500, len(id2doc)))\n            embeddings = self.embedder.encode(sample_docs, normalize_embeddings=True, convert_to_numpy=True)\n            centroids[domain_name] = embeddings.mean(axis=0)\n        return centroids\n    \n    def route_query(self, query: str, top_k: int = 2) -> List[str]:\n        query_emb = self.embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n        similarities = {domain: float(np.dot(query_emb, centroid)) for domain, centroid in self.domain_centroids.items()}\n        sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        selected = [d[0] for d in sorted_domains[:top_k]]\n        logger.info(f\"🧭 Routed to: {selected}\")\n        return selected\n\n# ============================================================================\n# RAG PIPELINE\n# ============================================================================\n\nclass MultiDomainRAGPipeline:\n    def __init__(self, config: RAGConfig, domains: List[DomainConfig]):\n        self.config = config\n        self.domains = domains\n        self.device = device\n        \n        self.model_manager = ModelManager(config, device)\n        self.models = self.model_manager.load_all()\n        \n        self.data = {}\n        self.test_data = {}\n        domain_chunks = {}\n        \n        for domain in domains:\n            train_data, test_data = DatasetLoader.load_domain_data(domain)\n            self.data[domain.name] = train_data\n            self.test_data[domain.name] = test_data\n            chunks = TextChunker.create_chunks(train_data, window=config.chunk_window, stride=config.chunk_stride)\n            domain_chunks[domain.name] = chunks\n        \n        self.index_manager = MultiDomainIndexManager(config, self.models['embedder'])\n        self.index_manager.load_all_domains(domain_chunks)\n        \n        self.router = QueryRouter(self.models['embedder'], self.index_manager.domain_indices)\n        self.prompts_log = []\n        \n        logger.info(\"✅ Multi-domain RAG pipeline initialized\")\n    \n    def generate_hyde_answer(self, query: str) -> str:\n        if self.models['hyde_model'] is None:\n            return query\n        \n        prompt = f\"Question: {query}\\nAnswer:\"\n        try:\n            inputs = self.models['hyde_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['hyde_model'].generate(\n                    **inputs, max_new_tokens=self.config.hyde_max_tokens,\n                    do_sample=False, pad_token_id=self.models['hyde_tokenizer'].eos_token_id,\n                    repetition_penalty=1.15\n                )\n            text = self.models['hyde_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            hyde = clean_text_artifacts(text.split(\"Answer:\")[-1])\n            return hyde if hyde else query\n        except:\n            return query\n    \n    def retrieve_from_domain(self, query: str, domain_name: str, k: int) -> List[Tuple[int, float, str]]:\n        domain_data = self.index_manager.domain_indices[domain_name]\n        index = domain_data['index']\n        id2doc = domain_data['id2doc']\n        bm25 = domain_data['bm25']\n        \n        hyde_text = self.generate_hyde_answer(query)\n        q_emb = self.models['embedder'].encode([query], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        h_emb = self.models['embedder'].encode([hyde_text], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        merged_emb = (1 - self.config.hyde_weight) * q_emb + self.config.hyde_weight * h_emb\n        \n        D, I = index.search(merged_emb, k)\n        faiss_scores = D[0]\n        if faiss_scores.max() > faiss_scores.min():\n            faiss_norm = (faiss_scores - faiss_scores.min()) / (faiss_scores.max() - faiss_scores.min())\n        else:\n            faiss_norm = np.ones_like(faiss_scores)\n        faiss_map = {int(idx): float(score) for idx, score in zip(I[0], faiss_norm)}\n        \n        bm25_scores = bm25.get_scores(word_tokenize(query.lower()))\n        if bm25_scores.max() > bm25_scores.min():\n            bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())\n        else:\n            bm25_norm = np.zeros_like(bm25_scores)\n        \n        candidates = set(I[0].tolist()) | set(np.argsort(bm25_scores)[::-1][:k].tolist())\n        merged_scores = []\n        for idx in candidates:\n            f = faiss_map.get(int(idx), 0.0)\n            b = float(bm25_norm[int(idx)]) if int(idx) < len(bm25_norm) else 0.0\n            score = self.config.faiss_alpha * f + (1 - self.config.faiss_alpha) * b\n            merged_scores.append((int(idx), score, domain_name))\n        \n        merged_scores.sort(key=lambda x: x[1], reverse=True)\n        return merged_scores[:k]\n    \n    def rerank_candidates(self, query: str, candidates: List[Tuple[int, float, str]]) -> List[Tuple[str, float, str]]:\n        texts, metadata = [], []\n        for idx, score, domain_name in candidates:\n            domain_data = self.index_manager.domain_indices[domain_name]\n            text = domain_data['id2doc'][idx]\n            texts.append(text)\n            metadata.append((idx, domain_name))\n        \n        reranker_scores = []\n        batch_size = 8\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = self.models['reranker_tokenizer'](\n                [query] * len(batch_texts), batch_texts,\n                padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n            ).to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.models['reranker_model'](**inputs)\n                logits = outputs.logits.cpu().numpy()\n            \n            for lg in logits:\n                if lg.shape == ():\n                    score = float(lg)\n                elif len(lg.shape) == 1 and lg.shape[0] == 1:\n                    score = float(lg[0])\n                elif len(lg.shape) == 1 and lg.shape[0] == 2:\n                    score = float(lg[1])\n                else:\n                    score = float(np.max(lg))\n                reranker_scores.append(score)\n        \n        reranked = [(texts[i], reranker_scores[i], metadata[i][1]) for i in range(len(texts))]\n        reranked.sort(key=lambda x: x[1], reverse=True)\n        return reranked[:self.config.rerank_topk]\n    \n    def generate_answer(self, query: str, contexts: List[Tuple[str, float, str]]) -> str:\n        context_parts = [f\"[Source {i+1} from {domain}]:\\n{text}\" \n                        for i, (text, score, domain) in enumerate(contexts[:self.config.context_chunks])]\n        context_block = \"\\n\\n\".join(context_parts)\n        \n        prompt = f\"\"\"Based on the following medical information, answer the question concisely and accurately.\n\n{context_block}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n        \n        try:\n            inputs = self.models['gen_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['gen_model'].generate(\n                    **inputs, max_new_tokens=self.config.max_new_tokens,\n                    do_sample=False, pad_token_id=self.models['gen_tokenizer'].eos_token_id,\n                    repetition_penalty=1.1\n                )\n            raw = self.models['gen_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            answer = clean_text_artifacts(raw.split(\"Answer:\")[-1])\n            \n            self.prompts_log.append({\n                \"type\": \"generate\", \"query\": query,\n                \"contexts\": [(t, d) for t, _, d in contexts[:self.config.context_chunks]],\n                \"prompt\": prompt, \"raw\": raw, \"answer\": answer, \"timestamp\": time.time()\n            })\n            \n            return answer if answer else \"Insufficient information.\"\n        except Exception as e:\n            logger.error(f\"Generation failed: {e}\")\n            return \"Error generating answer.\"\n    \n    def compute_metrics(self, query: str, answer: str, contexts: List[Tuple[str, float, str]]) -> Dict[str, float]:\n        metrics = {}\n        \n        if contexts:\n            retrieval_score = np.mean([score for _, score, _ in contexts[:self.config.context_chunks]])\n            metrics['retrieval'] = float(retrieval_score)\n        else:\n            metrics['retrieval'] = 0.0\n        \n        try:\n            context_texts = [text for text, _, _ in contexts[:self.config.context_chunks]]\n            all_keywords = []\n            \n            if self.models['keyword_extractor']:\n                for ctx_text in context_texts:\n                    keywords = self.models['keyword_extractor'].extract_keywords(\n                        ctx_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5\n                    )\n                    all_keywords.extend([kw for kw, _ in keywords])\n            \n            unique_keywords = list(dict.fromkeys([kw.lower() for kw in all_keywords if kw]))\n            \n            if unique_keywords and answer:\n                answer_emb = self.models['embedder'].encode([answer], normalize_embeddings=True, convert_to_tensor=True)\n                keyword_embs = self.models['embedder'].encode(unique_keywords, normalize_embeddings=True, convert_to_tensor=True)\n                similarities = util.cos_sim(answer_emb, keyword_embs).cpu().numpy()[0]\n                covered = (similarities >= self.config.completeness_threshold).sum()\n                metrics['completeness'] = float(covered / len(unique_keywords))\n            else:\n                metrics['completeness'] = 0.0\n        except:\n            metrics['completeness'] = 0.0\n        \n        try:\n            if answer and contexts:\n                answer_sentences = sent_tokenize(answer)\n                context_sentences = []\n                for text, _, _ in contexts[:self.config.context_chunks]:\n                    context_sentences.extend(sent_tokenize(text))\n                \n                if answer_sentences and context_sentences:\n                    ans_embs = self.models['embedder'].encode(answer_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    ctx_embs = self.models['embedder'].encode(context_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    sim_matrix = util.cos_sim(ans_embs, ctx_embs).cpu().numpy()\n                    max_sims = np.max(sim_matrix, axis=1)\n                    faithful = (max_sims >= self.config.faithfulness_threshold).sum()\n                    metrics['faithfulness'] = float(faithful / len(answer_sentences))\n                else:\n                    metrics['faithfulness'] = 0.0\n            else:\n                metrics['faithfulness'] = 0.0\n        except:\n            metrics['faithfulness'] = 0.0\n        \n        metrics['composite'] = (\n            self.config.retrieval_weight * metrics['retrieval'] +\n            self.config.completeness_weight * metrics['completeness'] +\n            self.config.faithfulness_weight * metrics['faithfulness']\n        )\n        \n        return metrics\n    \n    def run_query(self, query: str, top_domains: int = 2, log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"🔍 Processing: {query[:100]}...\")\n        \n        selected_domains = self.router.route_query(query, top_k=top_domains)\n        \n        all_candidates = []\n        for domain_name in selected_domains:\n            candidates = self.retrieve_from_domain(query, domain_name, k=self.config.retrieve_k)\n            all_candidates.extend(candidates)\n        \n        if log_diagnostics:\n            logger.info(f\"Retrieved {len(all_candidates)} candidates from {len(selected_domains)} domains\")\n        \n        reranked = self.rerank_candidates(query, all_candidates)\n        \n        if log_diagnostics:\n            logger.info(\"Top reranked contexts:\")\n            for i, (text, score, domain) in enumerate(reranked[:3]):\n                logger.info(f\"  {i+1}. [{domain}] (score={score:.3f}): {text[:150]}...\")\n        \n        answer = self.generate_answer(query, reranked)\n        metrics = self.compute_metrics(query, answer, reranked)\n        \n        result = {\n            \"query\": query,\n            \"routed_domains\": selected_domains,\n            \"answer\": answer,\n            \"contexts\": [(text, domain) for text, _, domain in reranked[:self.config.context_chunks]],\n            \"metrics\": metrics\n        }\n        \n        return result\n    \n    def evaluate_batch(self, queries: List[str], log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"📊 Evaluating {len(queries)} queries...\")\n        \n        results = []\n        failed = []\n        \n        for i, query in enumerate(queries):\n            try:\n                result = self.run_query(query, log_diagnostics=log_diagnostics)\n                results.append(result)\n                \n                if (i + 1) % 3 == 0:\n                    logger.info(f\"Progress: {i+1}/{len(queries)}\")\n                    monitor_memory()\n            except Exception as e:\n                logger.error(f\"Failed query {i}: {e}\")\n                failed.append((i, query, str(e)))\n        \n        if not results:\n            return {\"error\": \"No successful queries\"}\n        \n        avg_metrics = {\n            \"retrieval\": np.mean([r[\"metrics\"][\"retrieval\"] for r in results]),\n            \"completeness\": np.mean([r[\"metrics\"][\"completeness\"] for r in results]),\n            \"faithfulness\": np.mean([r[\"metrics\"][\"faithfulness\"] for r in results]),\n            \"composite\": np.mean([r[\"metrics\"][\"composite\"] for r in results])\n        }\n        \n        summary = {\n            \"total_queries\": len(queries),\n            \"successful\": len(results),\n            \"failed\": len(failed),\n            \"success_rate\": len(results) / len(queries),\n            \"average_metrics\": avg_metrics,\n            \"failed_queries\": failed,\n            \"individual_results\": results\n        }\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"evaluation_{timestamp}.json\"\n        try:\n            with open(results_file, \"w\") as f:\n                json.dump(summary, f, indent=2, default=str)\n            logger.info(f\"💾 Results saved to {results_file}\")\n        except:\n            pass\n        \n        return summary\n\n# ============================================================================\n# PIPELINE EXECUTION\n# ============================================================================\n\nlogger.info(\"=\"*80)\nlogger.info(\"🚀 INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\nlogger.info(\"=\"*80)\n\nrag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n\ntest_queries = [\n    \"What are the recommended health screenings for women in their 40s?\",\n    \"Explain the symptoms and management of preeclampsia.\",\n    \"What are the early warning signs of Parkinson's disease?\",\n    \"How is PCOS diagnosed and treated?\",\n    \"What are the differences between Type 1 and Type 2 diabetes?\"\n]\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"🧪 RUNNING SINGLE QUERY TEST WITH DIAGNOSTICS\")\nlogger.info(\"=\"*80)\n\ntest_query = test_queries[0]\nresult = rag_pipeline.run_query(test_query, top_domains=2, log_diagnostics=True)\n\nlogger.info(f\"\\n{'='*80}\")\nlogger.info(f\"📋 QUERY: {result['query']}\")\nlogger.info(f\"{'='*80}\")\nlogger.info(f\"🎯 Routed to: {result['routed_domains']}\")\nlogger.info(f\"\\n✅ ANSWER:\\n{result['answer']}\")\nlogger.info(f\"\\n📊 METRICS:\")\nfor metric_name, value in result['metrics'].items():\n    logger.info(f\"   {metric_name}: {value:.3f}\")\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"📊 RUNNING BATCH EVALUATION\")\nlogger.info(\"=\"*80)\n\nbatch_results = rag_pipeline.evaluate_batch(test_queries[:3], log_diagnostics=False)\n\nlogger.info(f\"\\n{'='*80}\")\nlogger.info(\"📈 BATCH EVALUATION SUMMARY\")\nlogger.info(f\"{'='*80}\")\nlogger.info(f\"Success Rate: {batch_results['success_rate']:.1%}\")\nlogger.info(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\nlogger.info(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\nlogger.info(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\nlogger.info(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"✅ MULTI-DOMAIN RAG PIPELINE COMPLETE\")\nlogger.info(\"=\"*80)\n\ntry:\n    with open(config.prompts_log, \"wb\") as f:\n        pickle.dump(rag_pipeline.prompts_log, f)\n    logger.info(f\"📝 Prompt logs saved to {config.prompts_log}\")\nexcept:\n    pass\n\nmonitor_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T17:06:45.864818Z","iopub.execute_input":"2025-10-26T17:06:45.865318Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76879ad3c5545c19a859b895443da1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ba0cfdbdef414f808dcc3ddd7a755d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8113af2c33145a99a1fe8607fa67192"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1adec0342b5d498797cca8e3a4c35b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a024d1f54241b7a2a76c9b618c250d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2733f10e394b4e4ea5c10db694fa04d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b75f5804a484dda8efe358ba58ac943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"246b22a10b8e4010a0f04fe29437d55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05e93831bb94d7eb7b58c0de918b5aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030d920f7b374b2381e05d4fcb147f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187b1a1d362845eb912c69870c189dd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2bbed82104347c4a91578a630c7f240"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2640af13de714c45953ed192743a3e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eafa54deb054cedb2a7af510cc8c4a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c3f928fde24740b639d2f2c4f1b416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944d7c83b01247e0a871d9e1117f7ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a7e1063952441f5981aa0b20dfe2644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a176be64744d57b4a567921625646b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71595f2503684eb1bd49d7b0a155fa73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e36e5b7d6fc434c82dd29172de93b9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9d3d43fa9a43e29f0d01cddcf9cbc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dcf9f70455e42739d7fd5d768dc4827"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a43331209285409589785abc6e7d34f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f70bbe713394e5f8918ca5352d7fc3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271953c0025a4ad091a57d1bd18ade6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5081e9ab41544791b599975a298708fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ba21e4ccbf49bc88c0de82290d3c4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b16bb3f3a149619a7aceae745fe43a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1500b598a243c1a721d9304955e314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7cc4f09bd0c479283021d7ed0378808"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}