{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13551782,"sourceType":"datasetVersion","datasetId":8606988}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================== CELL 1: FIX DEPENDENCIES ==========================\nimport subprocess\nimport sys\n\nprint('üîß Fixing all package dependencies and kernel state...')\n\n# STEP 1: Wipe any incompatible pyarrow\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], capture_output=True, check=False)\n\n# STEP 2: Install pyarrow and essentials ONLY once before next cell\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyarrow==15.0.2\", \"keybert\", \"rank-bm25\", \"evaluate\", \"faiss-cpu\", \"protobuf<5.0.0\", \"sacremoses\"], check=True)\n\n# STEP 3: Install bigframes dependencies if required by your notebook/business use\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rich==13.7.1\"], check=True)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"google-cloud-bigquery-storage>=2.30.0\"], check=True)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"google-cloud-bigquery>=3.31.0\"], check=True)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"google-api-core>=2.10.2\"], check=True)\n\nprint('‚úÖ Dependencies fixed and installed. Now RESTART the kernel and run Cell 2.')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== CELL 2: SANITY CHECK ALL IMPORTS =====================\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"üîç Verifying critical imports...\")\n\ntry:\n    from datasets import load_dataset\n    print(\"‚úÖ datasets\")\n    from sentence_transformers import SentenceTransformer\n    print(\"‚úÖ sentence-transformers\")\n    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n    print(\"‚úÖ transformers\")\n    import faiss\n    print(\"‚úÖ faiss\")\n    from keybert import KeyBERT\n    print(\"‚úÖ keybert\")\n    from rank_bm25 import BM25Okapi\n    print(\"‚úÖ rank-bm25\")\n    import torch\n    print(f\"‚úÖ torch (device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n    print(\"\\nüéâ ALL CRITICAL IMPORTS SUCCESSFUL - You can now run your full pipeline!\")\nexcept Exception as e:\n    print(f\"‚ùå Import failed: {e}\\nPlease RESTART THE KERNEL and run Cell 1 again.\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T05:45:48.863565Z","iopub.execute_input":"2025-10-30T05:45:48.863817Z","iopub.status.idle":"2025-10-30T05:46:31.573772Z","shell.execute_reply.started":"2025-10-30T05:45:48.863796Z","shell.execute_reply":"2025-10-30T05:46:31.573075Z"}},"outputs":[{"name":"stdout","text":"üîç Verifying critical imports...\n‚úÖ datasets\n","output_type":"stream"},{"name":"stderr","text":"2025-10-30 05:46:08.171842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761803168.589322     163 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761803168.708660     163 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"‚úÖ sentence-transformers\n‚úÖ transformers\n‚úÖ faiss\n‚úÖ keybert\n‚úÖ rank-bm25\n‚úÖ torch (device: cuda)\n\nüéâ ALL CRITICAL IMPORTS SUCCESSFUL - You can now run your full pipeline!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =================== FULL MULTI-DOMAIN RAG PIPELINE (CLEAN) ===================\n\nimport re\nimport json\nimport time\nimport pickle\nimport random\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Any\n\nimport numpy as np\nimport torch\nimport faiss\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    AutoModelForSequenceClassification\n)\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom rank_bm25 import BM25Okapi\nfrom keybert import KeyBERT\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"üîß Using device: {device}\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n@dataclass\nclass DomainConfig:\n    name: str\n    dataset_name: str\n    config_name: str = None\n    dataset_split: str = \"train\"\n    index_path: str = None\n    id2doc_path: str = None\n    metadata_path: str = None\n    \n    def __post_init__(self):\n        if self.index_path is None:\n            self.index_path = f\"{self.name}_faiss.index\"\n        if self.id2doc_path is None:\n            self.id2doc_path = f\"{self.name}_id2doc.pkl\"\n        if self.metadata_path is None:\n            self.metadata_path = f\"{self.name}_metadata.json\"\n\n@dataclass\nclass RAGConfig:\n    embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    reranker_model: str = \"BAAI/bge-reranker-large\"\n    hyde_model: str = \"microsoft/BioGPT-Large\"\n    generator_model: str = \"microsoft/BioGPT-Large\"\n    chunk_window: int = 3\n    chunk_stride: int = 1\n    retrieve_k: int = 30\n    rerank_topk: int = 8\n    context_chunks: int = 4\n    hyde_weight: float = 0.4\n    faiss_alpha: float = 0.6\n    max_new_tokens: int = 200\n    hyde_max_tokens: int = 60\n    completeness_threshold: float = 0.65\n    faithfulness_threshold: float = 0.55\n    retrieval_weight: float = 0.4\n    completeness_weight: float = 0.3\n    faithfulness_weight: float = 0.3\n    prompts_log: str = \"prompts_outputs.pkl\"\n    random_seed: int = 42\n    test_size: float = 0.15\n\nDOMAINS = [\n    DomainConfig(name=\"women_health\", dataset_name=\"altaidevorg/women-health-mini\"),\n    DomainConfig(name=\"medical_qa\", dataset_name=\"Malikeh1375/medical-question-answering-datasets\", config_name=\"all-processed\")\n]\nconfig = RAGConfig()\n\n# ============================================================================\n# UTILITY FUNCTIONS\n# ============================================================================\n\ndef clean_text_artifacts(text: str) -> str:\n    text = re.sub(r\"^(Answer:|Final answer:|Response:)\\s*\", \"\", text, flags=re.IGNORECASE)\n    text = re.sub(r\"<\\/?[^>]+>|</s>|‚ñÉ|\\[INST\\]|\\[/INST\\]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text.strip(\" \\n\\r\\t\\\"'\")\n\ndef monitor_memory():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        logger.info(f\"üíæ GPU: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n        if allocated/total > 0.85:\n            torch.cuda.empty_cache()\n\n# ============================================================================\n# DATASET LOADING\n# ============================================================================\n\nclass DatasetLoader:\n    @staticmethod\n    def extract_qa_pairs(dataset, domain_name: str) -> list:\n        qa_data = []\n        for idx, row in enumerate(dataset):\n            try:\n                # women-health conversations\n                if 'conversations' in row and isinstance(row['conversations'], list):\n                    conversations = row['conversations']\n                    question, answer = \"\", \"\"\n                    for msg in conversations:\n                        if msg.get(\"role\") == \"user\" and not question:\n                            question = msg.get(\"content\", \"\")\n                        if msg.get(\"role\") == \"assistant\" and not answer:\n                            answer = msg.get(\"content\", \"\")\n                    if question and answer:\n                        qa_data.append({\n                            \"question\": question.strip(),\n                            \"answer\": answer.strip(),\n                            \"domain\": domain_name,\n                            \"source_id\": idx\n                        })\n                        continue\n                # medical_qa and generic\n                if 'question' in row and 'answer' in row:\n                    qa_data.append({\n                        \"question\": str(row['question']).strip(),\n                        \"answer\": str(row['answer']).strip(),\n                        \"domain\": domain_name,\n                        \"source_id\": idx\n                    })\n                    continue\n                if 'input' in row and 'output' in row:\n                    qa_data.append({\n                        \"question\": str(row['input']).strip(),\n                        \"answer\": str(row['output']).strip(),\n                        \"domain\": domain_name,\n                        \"source_id\": idx\n                    })\n            except Exception as e:\n                if idx < 2:\n                    print(f\"extract_qa_pairs WARNING, row {idx}: {e}\")\n                continue\n        return qa_data\n    \n    @staticmethod\n    def load_domain_data(domain_config: DomainConfig) -> tuple:\n        logger.info(f\"üì• Loading {domain_config.name}...\")\n        try:\n            if domain_config.config_name:\n                dataset = load_dataset(domain_config.dataset_name, domain_config.config_name, split=domain_config.dataset_split)\n            else:\n                dataset = load_dataset(domain_config.dataset_name, split=domain_config.dataset_split)\n            logger.info(f\"Dataset {domain_config.name} loaded with {len(dataset)} rows\")\n            qa_data = DatasetLoader.extract_qa_pairs(dataset, domain_config.name)\n            if not qa_data:\n                logger.error(f\"No QA pairs extracted from {domain_config.name}\")\n                logger.error(f\"Sample row structure: {dataset[0]}\")\n                raise ValueError(f\"No QA pairs extracted from {domain_config.name}. Check dataset structure.\")\n            train_data, test_data = train_test_split(\n                qa_data, test_size=config.test_size, random_state=config.random_seed\n            )\n            logger.info(f\"‚úÖ {domain_config.name}: {len(train_data)} train, {len(test_data)} test\")\n            return train_data, test_data\n        except Exception as e:\n            logger.error(f\"‚ùå Failed to load {domain_config.name}: {e}\")\n            raise\n\n# ============================================================================\n# TEXT CHUNKING\n# ============================================================================\n\nclass TextChunker:\n    @staticmethod\n    def create_chunks(data: List[Dict], window: int = 3, stride: int = 1, min_chars: int = 50) -> List[Dict]:\n        chunks = []\n        for item in data:\n            text = item.get(\"answer\", \"\")\n            if not text or len(text) < min_chars:\n                continue\n            \n            sentences = sent_tokenize(text)\n            if not sentences:\n                continue\n            \n            if len(sentences) <= window:\n                chunks.append({\n                    \"chunk\": \" \".join(sentences),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks)\n                })\n                continue\n            \n            for i in range(0, max(1, len(sentences) - window + 1), stride):\n                chunks.append({\n                    \"chunk\": \" \".join(sentences[i:i + window]),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks),\n                    \"window\": (i, i + window)\n                })\n        return chunks\n\n# ============================================================================\n# MODEL MANAGEMENT\n# ============================================================================\n\nclass ModelManager:\n    def __init__(self, config: RAGConfig, device: torch.device):\n        self.config = config\n        self.device = device\n        self.models = {}\n    \n    def load_embedder(self):\n        logger.info(f\"üì¶ Loading embedder...\")\n        embedder = SentenceTransformer(self.config.embed_model, device=self.device)\n        self.models['embedder'] = embedder\n        logger.info(f\"‚úÖ Embedder loaded\")\n        return embedder\n    \n    def load_reranker(self):\n        logger.info(f\"üì¶ Loading reranker...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.reranker_model)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            self.config.reranker_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        ).to(self.device)\n        model.eval()\n        self.models['reranker_tokenizer'] = tokenizer\n        self.models['reranker_model'] = model\n        logger.info(f\"‚úÖ Reranker loaded\")\n        return tokenizer, model\n    \n    def load_hyde_model(self):\n        logger.info(f\"üì¶ Loading HyDE model...\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(self.config.hyde_model)\n            model = AutoModelForCausalLM.from_pretrained(\n                self.config.hyde_model,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                low_cpu_mem_usage=True\n            ).to(self.device)\n            model.eval()\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            self.models['hyde_tokenizer'] = tokenizer\n            self.models['hyde_model'] = model\n            logger.info(f\"‚úÖ HyDE model loaded\")\n            return tokenizer, model\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è HyDE load failed, using query expansion: {e}\")\n            return None, None\n    \n    def load_generator(self):\n        logger.info(f\"üì¶ Loading generator...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.generator_model)\n        model = AutoModelForCausalLM.from_pretrained(\n            self.config.generator_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            low_cpu_mem_usage=True\n        ).to(self.device)\n        model.eval()\n        self.models['gen_tokenizer'] = tokenizer\n        self.models['gen_model'] = model\n        logger.info(f\"‚úÖ Generator loaded\")\n        return tokenizer, model\n    \n    def load_keyword_extractor(self):\n        try:\n            kw_model = KeyBERT(model=self.models.get('embedder'))\n            self.models['keyword_extractor'] = kw_model\n            logger.info(f\"‚úÖ KeyBERT loaded\")\n            return kw_model\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è KeyBERT load failed: {e}\")\n            return None\n    \n    def load_all(self):\n        logger.info(\"üîß Loading all models...\")\n        self.load_embedder()\n        self.load_reranker()\n        self.load_hyde_model()\n        self.load_generator()\n        self.load_keyword_extractor()\n        monitor_memory()\n        logger.info(\"‚úÖ All models loaded\")\n        return self.models\n\n# ============================================================================\n# INDEX MANAGEMENT\n# ============================================================================\n\nclass MultiDomainIndexManager:\n    def __init__(self, config: RAGConfig, embedder: SentenceTransformer):\n        self.config = config\n        self.embedder = embedder\n        self.domain_indices = {}\n    \n    def build_or_load_domain_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        if Path(domain_config.index_path).exists() and Path(domain_config.id2doc_path).exists():\n            try:\n                return self._load_existing_index(domain_config)\n            except:\n                pass\n        return self._build_new_index(domain_config, chunks)\n    \n    def _load_existing_index(self, domain_config: DomainConfig) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"üìÇ Loading existing {domain_config.name} index...\")\n        index = faiss.read_index(domain_config.index_path)\n        with open(domain_config.id2doc_path, \"rb\") as f:\n            id2doc = pickle.load(f)\n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        logger.info(f\"‚úÖ Loaded {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def _build_new_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"üî® Building {domain_config.name} index...\")\n        id2doc = [chunk[\"chunk\"] for chunk in chunks]\n        \n        embeddings = self.embedder.encode(\n            id2doc, normalize_embeddings=True, show_progress_bar=True,\n            batch_size=64, convert_to_numpy=True\n        ).astype('float32')\n        \n        dim = embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(embeddings)\n        \n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        \n        faiss.write_index(index, domain_config.index_path)\n        with open(domain_config.id2doc_path, \"wb\") as f:\n            pickle.dump(id2doc, f)\n        \n        metadata = {\"created_at\": time.time(), \"n_vectors\": int(index.ntotal), \"embedding_dim\": dim, \"domain\": domain_config.name}\n        with open(domain_config.metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        \n        logger.info(f\"‚úÖ Built {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def load_all_domains(self, domain_chunks: Dict[str, List[Dict]]):\n        for domain in DOMAINS:\n            index, id2doc, bm25 = self.build_or_load_domain_index(domain, domain_chunks.get(domain.name, []))\n            self.domain_indices[domain.name] = {\n                'index': index, 'id2doc': id2doc, 'bm25': bm25, 'config': domain\n            }\n        logger.info(f\"‚úÖ Loaded {len(self.domain_indices)} domain indices\")\n\n# ============================================================================\n# QUERY ROUTER\n# ============================================================================\n\nclass QueryRouter:\n    def __init__(self, embedder: SentenceTransformer, domain_indices: Dict):\n        self.embedder = embedder\n        self.domain_indices = domain_indices\n        self.domain_centroids = self._compute_centroids()\n    \n    def _compute_centroids(self) -> Dict[str, np.ndarray]:\n        centroids = {}\n        logger.info(\"üéØ Computing domain centroids...\")\n        for domain_name, domain_data in self.domain_indices.items():\n            id2doc = domain_data['id2doc']\n            sample_docs = random.sample(id2doc, min(500, len(id2doc)))\n            embeddings = self.embedder.encode(sample_docs, normalize_embeddings=True, convert_to_numpy=True)\n            centroids[domain_name] = embeddings.mean(axis=0)\n        return centroids\n    \n    def route_query(self, query: str, top_k: int = 2) -> List[str]:\n        query_emb = self.embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n        similarities = {domain: float(np.dot(query_emb, centroid)) for domain, centroid in self.domain_centroids.items()}\n        sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        selected = [d[0] for d in sorted_domains[:top_k]]\n        logger.info(f\"üß≠ Routed to: {selected}\")\n        return selected\n\n# ============================================================================\n# MULTI-DOMAIN RAG PIPELINE\n# ============================================================================\n\nclass MultiDomainRAGPipeline:\n    def __init__(self, config: RAGConfig, domains: List[DomainConfig]):\n        self.config = config\n        self.domains = domains\n        self.device = device\n        \n        self.model_manager = ModelManager(config, device)\n        self.models = self.model_manager.load_all()\n        \n        self.data = {}\n        self.test_data = {}\n        domain_chunks = {}\n        \n        for domain in domains:\n            train_data, test_data = DatasetLoader.load_domain_data(domain)\n            self.data[domain.name] = train_data\n            self.test_data[domain.name] = test_data\n            chunks = TextChunker.create_chunks(train_data, window=config.chunk_window, stride=config.chunk_stride)\n            domain_chunks[domain.name] = chunks\n        \n        self.index_manager = MultiDomainIndexManager(config, self.models['embedder'])\n        self.index_manager.load_all_domains(domain_chunks)\n        \n        self.router = QueryRouter(self.models['embedder'], self.index_manager.domain_indices)\n        self.prompts_log = []\n        \n        logger.info(\"‚úÖ Multi-domain RAG pipeline initialized\")\n    \n    def generate_hyde_answer(self, query: str) -> str:\n        if self.models['hyde_model'] is None:\n            return query\n        \n        prompt = f\"Question: {query}\\nAnswer:\"\n        try:\n            inputs = self.models['hyde_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['hyde_model'].generate(\n                    **inputs, max_new_tokens=self.config.hyde_max_tokens,\n                    do_sample=False, pad_token_id=self.models['hyde_tokenizer'].eos_token_id,\n                    repetition_penalty=1.15\n                )\n            text = self.models['hyde_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            hyde = clean_text_artifacts(text.split(\"Answer:\")[-1])\n            return hyde if hyde else query\n        except:\n            return query\n    \n    def retrieve_from_domain(self, query: str, domain_name: str, k: int) -> List[Tuple[int, float, str]]:\n        domain_data = self.index_manager.domain_indices[domain_name]\n        index = domain_data['index']\n        id2doc = domain_data['id2doc']\n        bm25 = domain_data['bm25']\n        \n        hyde_text = self.generate_hyde_answer(query)\n        q_emb = self.models['embedder'].encode([query], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        h_emb = self.models['embedder'].encode([hyde_text], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        merged_emb = (1 - self.config.hyde_weight) * q_emb + self.config.hyde_weight * h_emb\n        \n        D, I = index.search(merged_emb, k)\n        faiss_scores = D[0]\n        if faiss_scores.max() > faiss_scores.min():\n            faiss_norm = (faiss_scores - faiss_scores.min()) / (faiss_scores.max() - faiss_scores.min())\n        else:\n            faiss_norm = np.ones_like(faiss_scores)\n        faiss_map = {int(idx): float(score) for idx, score in zip(I[0], faiss_norm)}\n        \n        bm25_scores = bm25.get_scores(word_tokenize(query.lower()))\n        if bm25_scores.max() > bm25_scores.min():\n            bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())\n        else:\n            bm25_norm = np.zeros_like(bm25_scores)\n        \n        candidates = set(I[0].tolist()) | set(np.argsort(bm25_scores)[::-1][:k].tolist())\n        merged_scores = []\n        for idx in candidates:\n            f = faiss_map.get(int(idx), 0.0)\n            b = float(bm25_norm[int(idx)]) if int(idx) < len(bm25_norm) else 0.0\n            score = self.config.faiss_alpha * f + (1 - self.config.faiss_alpha) * b\n            merged_scores.append((int(idx), score, domain_name))\n        \n        merged_scores.sort(key=lambda x: x[1], reverse=True)\n        return merged_scores[:k]\n    \n    def rerank_candidates(self, query: str, candidates: List[Tuple[int, float, str]]) -> List[Tuple[str, float, str]]:\n        texts, metadata = [], []\n        for idx, score, domain_name in candidates:\n            domain_data = self.index_manager.domain_indices[domain_name]\n            text = domain_data['id2doc'][idx]\n            texts.append(text)\n            metadata.append((idx, domain_name))\n        \n        reranker_scores = []\n        batch_size = 8\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = self.models['reranker_tokenizer'](\n                [query] * len(batch_texts), batch_texts,\n                padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n            ).to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.models['reranker_model'](**inputs)\n                logits = outputs.logits.cpu().numpy()\n            \n            for lg in logits:\n                if lg.shape == ():\n                    score = float(lg)\n                elif len(lg.shape) == 1 and lg.shape[0] == 1:\n                    score = float(lg[0])\n                elif len(lg.shape) == 1 and lg.shape[0] == 2:\n                    score = float(lg[1])\n                else:\n                    score = float(np.max(lg))\n                reranker_scores.append(score)\n        \n        reranked = [(texts[i], reranker_scores[i], metadata[i][1]) for i in range(len(texts))]\n        reranked.sort(key=lambda x: x[1], reverse=True)\n        return reranked[:self.config.rerank_topk]\n    \n    def generate_answer(self, query: str, contexts: List[Tuple[str, float, str]]) -> str:\n        context_parts = [f\"[Source {i+1} from {domain}]:\\n{text}\" \n                        for i, (text, score, domain) in enumerate(contexts[:self.config.context_chunks])]\n        context_block = \"\\n\\n\".join(context_parts)\n        \n        prompt = f\"\"\"Based on the following medical information, answer the question concisely and accurately.\n\n{context_block}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n        \n        try:\n            inputs = self.models['gen_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['gen_model'].generate(\n                    **inputs, max_new_tokens=self.config.max_new_tokens,\n                    do_sample=False, pad_token_id=self.models['gen_tokenizer'].eos_token_id,\n                    repetition_penalty=1.1\n                )\n            raw = self.models['gen_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            answer = clean_text_artifacts(raw.split(\"Answer:\")[-1])\n            \n            self.prompts_log.append({\n                \"type\": \"generate\", \"query\": query,\n                \"contexts\": [(t, d) for t, _, d in contexts[:self.config.context_chunks]],\n                \"prompt\": prompt, \"raw\": raw, \"answer\": answer, \"timestamp\": time.time()\n            })\n            \n            return answer if answer else \"Insufficient information.\"\n        except Exception as e:\n            logger.error(f\"Generation failed: {e}\")\n            return \"Error generating answer.\"\n    \n    def compute_metrics(self, query: str, answer: str, contexts: List[Tuple[str, float, str]]) -> Dict[str, float]:\n        metrics = {}\n        \n        if contexts:\n            retrieval_score = np.mean([score for _, score, _ in contexts[:self.config.context_chunks]])\n            metrics['retrieval'] = float(retrieval_score)\n        else:\n            metrics['retrieval'] = 0.0\n        \n        try:\n            context_texts = [text for text, _, _ in contexts[:self.config.context_chunks]]\n            all_keywords = []\n            \n            if self.models['keyword_extractor']:\n                for ctx_text in context_texts:\n                    keywords = self.models['keyword_extractor'].extract_keywords(\n                        ctx_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5\n                    )\n                    all_keywords.extend([kw for kw, _ in keywords])\n            \n            unique_keywords = list(dict.fromkeys([kw.lower() for kw in all_keywords if kw]))\n            \n            if unique_keywords and answer:\n                answer_emb = self.models['embedder'].encode([answer], normalize_embeddings=True, convert_to_tensor=True)\n                keyword_embs = self.models['embedder'].encode(unique_keywords, normalize_embeddings=True, convert_to_tensor=True)\n                similarities = util.cos_sim(answer_emb, keyword_embs).cpu().numpy()[0]\n                covered = (similarities >= self.config.completeness_threshold).sum()\n                metrics['completeness'] = float(covered / len(unique_keywords))\n            else:\n                metrics['completeness'] = 0.0\n        except:\n            metrics['completeness'] = 0.0\n        \n        try:\n            if answer and contexts:\n                answer_sentences = sent_tokenize(answer)\n                context_sentences = []\n                for text, _, _ in contexts[:self.config.context_chunks]:\n                    context_sentences.extend(sent_tokenize(text))\n                \n                if answer_sentences and context_sentences:\n                    ans_embs = self.models['embedder'].encode(answer_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    ctx_embs = self.models['embedder'].encode(context_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    sim_matrix = util.cos_sim(ans_embs, ctx_embs).cpu().numpy()\n                    max_sims = np.max(sim_matrix, axis=1)\n                    faithful = (max_sims >= self.config.faithfulness_threshold).sum()\n                    metrics['faithfulness'] = float(faithful / len(answer_sentences))\n                else:\n                    metrics['faithfulness'] = 0.0\n            else:\n                metrics['faithfulness'] = 0.0\n        except:\n            metrics['faithfulness'] = 0.0\n        \n        metrics['composite'] = (\n            self.config.retrieval_weight * metrics['retrieval'] +\n            self.config.completeness_weight * metrics['completeness'] +\n            self.config.faithfulness_weight * metrics['faithfulness']\n        )\n        \n        return metrics\n    \n    def run_query(self, query: str, top_domains: int = 2, log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"üîç Processing: {query[:100]}...\")\n        \n        selected_domains = self.router.route_query(query, top_k=top_domains)\n        \n        all_candidates = []\n        for domain_name in selected_domains:\n            candidates = self.retrieve_from_domain(query, domain_name, k=self.config.retrieve_k)\n            all_candidates.extend(candidates)\n        \n        if log_diagnostics:\n            logger.info(f\"Retrieved {len(all_candidates)} candidates from {len(selected_domains)} domains\")\n        \n        reranked = self.rerank_candidates(query, all_candidates)\n        \n        if log_diagnostics:\n            logger.info(\"Top reranked contexts:\")\n            for i, (text, score, domain) in enumerate(reranked[:3]):\n                logger.info(f\"  {i+1}. [{domain}] (score={score:.3f}): {text[:150]}...\")\n        \n        answer = self.generate_answer(query, reranked)\n        metrics = self.compute_metrics(query, answer, reranked)\n        \n        result = {\n            \"query\": query,\n            \"routed_domains\": selected_domains,\n            \"answer\": answer,\n            \"contexts\": [(text, domain) for text, _, domain in reranked[:self.config.context_chunks]],\n            \"metrics\": metrics\n        }\n        \n        return result\n    \n    def evaluate_batch(self, queries: List[str], log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"üìä Evaluating {len(queries)} queries...\")\n        \n        results = []\n        failed = []\n        \n        for i, query in enumerate(queries):\n            try:\n                result = self.run_query(query, log_diagnostics=log_diagnostics)\n                results.append(result)\n                \n                if (i + 1) % 3 == 0:\n                    logger.info(f\"Progress: {i+1}/{len(queries)}\")\n                    monitor_memory()\n            except Exception as e:\n                logger.error(f\"Failed query {i}: {e}\")\n                failed.append((i, query, str(e)))\n        \n        if not results:\n            return {\"error\": \"No successful queries\"}\n        \n        avg_metrics = {\n            \"retrieval\": np.mean([r[\"metrics\"][\"retrieval\"] for r in results]),\n            \"completeness\": np.mean([r[\"metrics\"][\"completeness\"] for r in results]),\n            \"faithfulness\": np.mean([r[\"metrics\"][\"faithfulness\"] for r in results]),\n            \"composite\": np.mean([r[\"metrics\"][\"composite\"] for r in results])\n        }\n        \n        summary = {\n            \"total_queries\": len(queries),\n            \"successful\": len(results),\n            \"failed\": len(failed),\n            \"success_rate\": len(results) / len(queries),\n            \"average_metrics\": avg_metrics,\n            \"failed_queries\": failed,\n            \"individual_results\": results\n        }\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"evaluation_{timestamp}.json\"\n        try:\n            with open(results_file, \"w\") as f:\n                json.dump(summary, f, indent=2, default=str)\n            logger.info(f\"üíæ Results saved to {results_file}\")\n        except:\n            pass\n        \n        return summary\n\n# ============================================================================\n# EXECUTION\n# ============================================================================\n\n# logger.info(\"=\"*80)\n# logger.info(\"üöÄ INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\n# logger.info(\"=\"*80)\n\n# rag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n\n# test_queries = [\n#     \"What are the recommended health screenings for women in their 40s?\",\n#     \"Explain the symptoms and management of preeclampsia.\",\n#     \"What are the early warning signs of Parkinson's disease?\",\n#     \"How is PCOS diagnosed and treated?\",\n#     \"What are the differences between Type 1 and Type 2 diabetes?\"\n# ]\n\n# result = rag_pipeline.run_query(test_queries[0], top_domains=2, log_diagnostics=True)\n# logger.info(f\"\\n==================\\nQUERY: {result['query']}\\n==================\")\n# logger.info(f\"Routed to: {result['routed_domains']}\\nANSWER:\\n{result['answer']}\")\n# logger.info(\"üìä METRICS:\")\n# for metric_name, value in result['metrics'].items():\n#     logger.info(f\" {metric_name}: {value:.3f}\")\n\n# logger.info(\"üìä RUNNING BATCH EVALUATION\")\n# batch_results = rag_pipeline.evaluate_batch(test_queries[:3], log_diagnostics=False)\n# logger.info(\"üìà BATCH EVALUATION SUMMARY\")\n# logger.info(f\"Success Rate: {batch_results['success_rate']:.1%}\")\n# logger.info(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\n# logger.info(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\n# logger.info(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\n# logger.info(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n# logger.info(\"‚úÖ MULTI-DOMAIN RAG PIPELINE COMPLETE\")\n\n# try:\n#     with open(config.prompts_log, \"wb\") as f:\n#         pickle.dump(rag_pipeline.prompts_log, f)\n#     logger.info(f\"üìù Prompt logs saved to {config.prompts_log}\")\n# except Exception:\n#     pass\n\n# monitor_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T05:47:19.927931Z","iopub.execute_input":"2025-10-30T05:47:19.928449Z","iopub.status.idle":"2025-10-30T05:47:20.387420Z","shell.execute_reply.started":"2025-10-30T05:47:19.928424Z","shell.execute_reply":"2025-10-30T05:47:20.386730Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# # --- Pipeline Execution Block (for final results output) ---\n\n# print(\"=\"*80)\n# print(\"üöÄ INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\n# print(\"=\"*80)\n\n# rag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n\n# test_queries = [\n#     \"What are the recommended health screenings for women in their 40s?\",\n#     \"Explain the symptoms and management of preeclampsia.\",\n#     \"What are the early warning signs of Parkinson's disease?\",\n#     \"How is PCOS diagnosed and treated?\",\n#     \"What are the differences between Type 1 and Type 2 diabetes?\"\n# ]\n\n# result = rag_pipeline.run_query(test_queries[0], top_domains=2, log_diagnostics=True)\n# print(\"\\n==================\\nQUERY:\", result['query'], \"\\n==================\")\n# print(\"Routed to:\", result['routed_domains'])\n# print(\"HYDE:\", rag_pipeline.generate_hyde_answer(result['query']))  # Optional: show HyDE draft\n# print(\"Answer:\", result['answer'])\n# print(\"üìä METRICS:\")\n# for metric_name, value in result['metrics'].items():\n#     print(f\" {metric_name}: {value:.3f}\")\n\n# print(\"\\nüìä RUNNING BATCH EVALUATION\")\n# batch_results = rag_pipeline.evaluate_batch(test_queries[:3], log_diagnostics=False)\n# print(\"üìà BATCH EVALUATION SUMMARY\")\n# print(f\"Success Rate: {batch_results['success_rate']:.1%}\")\n# print(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\n# print(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\n# print(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\n# print(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n# print(\"‚úÖ MULTI-DOMAIN RAG PIPELINE COMPLETE\")\n\n# try:\n#     with open(config.prompts_log, \"wb\") as f:\n#         pickle.dump(rag_pipeline.prompts_log, f)\n#     print(f\"üìù Prompt logs saved to {config.prompts_log}\")\n# except Exception:\n#     pass\n\n# monitor_memory()\n# ============================================================================\n# EXECUTION - PROFESSIONAL VERSION WITH FLEXIBLE INPUT\n# ============================================================================\n\ndef load_queries_from_file(file_path: str) -> List[str]:\n    \"\"\"Load queries from a text file (one query per line).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            queries = [line.strip() for line in f if line.strip()]\n        logger.info(f\"‚úÖ Loaded {len(queries)} queries from {file_path}\")\n        return queries\n    except Exception as e:\n        logger.error(f\"‚ùå Failed to load queries from {file_path}: {e}\")\n        return []\n\ndef run_interactive_mode(pipeline):\n    \"\"\"Interactive mode: user enters queries one by one.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üéØ INTERACTIVE QUERY MODE\")\n    print(\"=\"*80)\n    print(\"Enter your medical queries (type 'quit' or 'exit' to stop):\\n\")\n    \n    while True:\n        query = input(\"Query: \").strip()\n        if query.lower() in ['quit', 'exit', 'q']:\n            print(\"üëã Exiting interactive mode.\")\n            break\n        if not query:\n            continue\n            \n        result = pipeline.run_query(query, top_domains=2, log_diagnostics=False)\n        print(\"\\n\" + \"-\"*80)\n        print(f\"QUERY: {result['query']}\")\n        print(f\"Routed to: {result['routed_domains']}\")\n        print(f\"Answer: {result['answer']}\")\n        print(\"\\nüìä METRICS:\")\n        for metric_name, value in result['metrics'].items():\n            print(f\"  {metric_name}: {value:.3f}\")\n        print(\"-\"*80 + \"\\n\")\n\ndef run_batch_from_list(pipeline, queries: List[str], show_individual: bool = True):\n    \"\"\"Run batch evaluation on a list of queries.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä BATCH EVALUATION MODE ({len(queries)} queries)\")\n    print(\"=\"*80 + \"\\n\")\n    \n    if show_individual:\n        for i, query in enumerate(queries, 1):\n            print(f\"\\n[Query {i}/{len(queries)}]\")\n            result = pipeline.run_query(query, top_domains=2, log_diagnostics=False)\n            print(f\"Q: {result['query']}\")\n            print(f\"Routed to: {result['routed_domains']}\")\n            print(f\"A: {result['answer']}\")\n            print(\"Metrics:\", end=\" \")\n            for metric_name, value in result['metrics'].items():\n                print(f\"{metric_name}={value:.3f}\", end=\" \")\n            print(\"\\n\" + \"-\"*80)\n    \n    # Batch summary\n    batch_results = pipeline.evaluate_batch(queries, log_diagnostics=False)\n    print(\"\\nüìà BATCH EVALUATION SUMMARY\")\n    print(\"=\"*80)\n    print(f\"Success Rate: {batch_results['success_rate']:.1%}\")\n    print(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\n    print(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\n    print(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\n    print(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n    print(\"=\"*80)\n    \n    return batch_results\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    print(\"=\"*80)\n    print(\"üöÄ INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\n    print(\"=\"*80)\n    \n    # Initialize pipeline\n    rag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n    \n    # ========================================================================\n    # CONFIGURATION: Choose your execution mode\n    # ========================================================================\n    \n    # Mode 1: Interactive (user input)\n    RUN_INTERACTIVE = False\n    \n    # Mode 2: Batch from file\n    RUN_FROM_FILE = False\n    QUERIES_FILE = \"queries.txt\"  # One query per line\n    \n    # Mode 3: Batch from predefined list (for demo/testing)\n    RUN_DEMO_BATCH = True\n    DEMO_QUERIES = [\n        \"What are the recommended health screenings for women in their 40s?\",\n        \"Explain the symptoms and management of preeclampsia.\",\n        \"What are the early warning signs of Parkinson's disease?\",\n        \"How is PCOS diagnosed and treated?\",\n        \"What are the differences between Type 1 and Type 2 diabetes?\"\n    ]\n    \n    # Mode 4: Single query demo\n    RUN_SINGLE_DEMO = False\n    SINGLE_QUERY = \"What are the symptoms of menopause?\"\n    \n    # ========================================================================\n    # EXECUTE BASED ON CONFIGURATION\n    # ========================================================================\n    \n    if RUN_INTERACTIVE:\n        run_interactive_mode(rag_pipeline)\n    \n    elif RUN_FROM_FILE:\n        queries = load_queries_from_file(QUERIES_FILE)\n        if queries:\n            batch_results = run_batch_from_list(rag_pipeline, queries, show_individual=True)\n    \n    elif RUN_DEMO_BATCH:\n        batch_results = run_batch_from_list(rag_pipeline, DEMO_QUERIES, show_individual=True)\n    \n    elif RUN_SINGLE_DEMO:\n        result = rag_pipeline.run_query(SINGLE_QUERY, top_domains=2, log_diagnostics=True)\n        print(\"\\n==================\")\n        print(f\"QUERY: {result['query']}\")\n        print(\"==================\")\n        print(f\"Routed to: {result['routed_domains']}\")\n        print(f\"HYDE: {rag_pipeline.generate_hyde_answer(result['query'])}\")\n        print(f\"Answer: {result['answer']}\")\n        print(\"\\nüìä METRICS:\")\n        for metric_name, value in result['metrics'].items():\n            print(f\"  {metric_name}: {value:.3f}\")\n    \n    else:\n        print(\"‚ö†Ô∏è No execution mode selected. Set one of the RUN_* flags to True.\")\n    \n    # ========================================================================\n    # SAVE LOGS\n    # ========================================================================\n    \n    try:\n        with open(config.prompts_log, \"wb\") as f:\n            pickle.dump(rag_pipeline.prompts_log, f)\n        print(f\"\\nüìù Prompt logs saved to {config.prompts_log}\")\n    except Exception as e:\n        logger.warning(f\"Could not save prompt logs: {e}\")\n    \n    print(\"\\n‚úÖ MULTI-DOMAIN RAG PIPELINE COMPLETE\")\n    monitor_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T05:47:23.747638Z","iopub.execute_input":"2025-10-30T05:47:23.748308Z","iopub.status.idle":"2025-10-30T06:09:35.231688Z","shell.execute_reply.started":"2025-10-30T05:47:23.748283Z","shell.execute_reply":"2025-10-30T06:09:35.230963Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüöÄ INITIALIZING MULTI-DOMAIN RAG PIPELINE\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbfa40bd81244587b80f2825cea535fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3adf4ed2b745e1bc5e5a8406914c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f5ef0e745d472fb4cffbbcb62c1cfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8d5ac4e25048d9ae291300794cc22a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ec13d3d43424b3e8af4ff1940a506ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993d8f922d3343e781284cc7354c8ade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80bb49abd36a4a0da8001b0c79b242e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b528863fcd402bb9a764fcf1e01e81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d20ff5d031b405491071cd98ebb01ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af8a507a68e4587aed0e7d22976ccb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7629752c2dac4e45928af9e368791b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f0f558e4a647c99fffe16a1d939403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55fa68c53094734a4874f12da4c931c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c8f6c2cbdc48cfa8d710b9d5846241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c164278ef9994e919eb91348457decbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7267f79021047d49ea4cb254ab0494a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb7022d0c694760ba0b6301006f7f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095457f8142e4bf4a8a66cbc0efdbe8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e31772bb49d4d698cfd80968a455944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a57003f45d486f99c4131709762731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a694947ca754a2ca50be073469d5fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab6e401edb84bb285258a33f0914222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe197b9b2294cefabdd54c58ed37766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a94352af0764ddd8ffd5fbc5a525720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43fbbf92332e4b4491594a5ccaccbe4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a98d12167b4905a117802315941afd"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"women-health-mini.jsonl:   0%|          | 0.00/35.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9fd2ae722b24120a8e691e5dda85e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10348 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9cf5b44f264cc7b3a2d639244ef78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c897fa61eb544ab3aa2f1990d8ecc434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all-processed/train-00000-of-00001-9bfe4(‚Ä¶):   0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a97fe462572c4c1ca56222d29147ec03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/246678 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc6c2b5a9e0465095ccd059d01fc31a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3693 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e3f02bdf5b45ffadbdb01452681b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/12142 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b2e5c5879e480fbbe8456bf7eec56e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a712712f7847d584da3fc88d612a71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b4d040bc684203b0485ee670dd6c89"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nüìä BATCH EVALUATION MODE (5 queries)\n================================================================================\n\n\n[Query 1/5]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e065548b871a4719ab87158443c64f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cab9b6a3ce48c58a523fa3630c3fe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758ec1ed9d4b43aaa2e796ccb1b9d03e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2be9b1ffdc4940818b0970e1ce28045e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4827812f70bf49939c8725213959912c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de2775d580f489a9579a1eb3a3abd9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf95e147545463cbcc480be74f9ff11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0a3ee6000e4f8eb90f44c3426a25dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b077f691364432a3e41d7835de3d62"}},"metadata":{}},{"name":"stdout","text":"Q: What are the recommended health screenings for women in their 40s?\nRouted to: ['women_health', 'medical_qa']\nA: Pap smear every 3 years. The patient's response was ‚ÄúYes ‚Äù. The patient's response was‚Äú No ‚Äù. The patient's response was ‚ÄúNo ‚Äù. The patient's response was‚Äú Yes ‚Äù. The patient's response was ‚ÄúNo ‚Äù. The patient's response was‚Äú Yes ‚Äù. The patient's response was ‚ÄúNo ‚Äù. The patient's response was‚Äú Yes ‚Äù. < /\nMetrics: retrieval=1.050 completeness=0.143 faithfulness=0.100 composite=0.493 \n--------------------------------------------------------------------------------\n\n[Query 2/5]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0fcbcb01a148fe83761104a5e47bb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a3d0f1eb7349dc95bc3acaf89b19af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e4a72a4ea3a47b7b8c9830412730713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83832202680c4a94a8a34fe086526b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee9ba9cfae847faabf7c34d08d04549"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f6729b51be4fa49ce6df9725846627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba3e3bcb2724f97be718dc11934a6e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4feb01cf672f4dd98ed0bfaf7ddfcb96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7c260f8f6247f6913a1f487efaac44"}},"metadata":{}},{"name":"stdout","text":"Q: Explain the symptoms and management of preeclampsia.\nRouted to: ['women_health', 'medical_qa']\nA: Preeclampsia is a serious condition that affects both the mother and baby. It is important to recognize signs and symptoms of preeclampsia so that appropriate care can be provided.\nMetrics: retrieval=2.478 completeness=0.526 faithfulness=1.000 composite=1.449 \n--------------------------------------------------------------------------------\n\n[Query 3/5]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e413e726f2c8490d8c99ded126cdea34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6022c57f8d0b4e38abb298567685dbab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5baa0ac42b44a48af7ba621e3efbdf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"242170b0326c4db8884b560598f71852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"defb10686af941128150801c301333e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fa746c96d2f45208ac28270e3753c17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96dec09091674732a2208f83611026d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e473584333f2413ea5ae7821b9d750a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668a26a9250a485fa94d2d25ce39bb6b"}},"metadata":{}},{"name":"stdout","text":"Q: What are the early warning signs of Parkinson's disease?\nRouted to: ['medical_qa', 'women_health']\nA: The patient may notice some of the following symptoms: Tremor, rigidity, bradykinesia, postural instability, and resting tremor.\nMetrics: retrieval=3.458 completeness=0.250 faithfulness=1.000 composite=1.758 \n--------------------------------------------------------------------------------\n\n[Query 4/5]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6872829f4cef44819e5afb31b66c5a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b0c94292cc4d7c8b8b0117de59f123"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"218b798b340e449ea2fa20e6e7ed1fe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876b9cc9191149a7a7f6656fb0017276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276d8810e83a40db94eb80f3e5f5c9d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6d5d5960a2e4620918ce6e6700a7ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9280b25afcef4624bd45e05944666a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8bf234bf936453a9cf4cf775e2b013b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2929ac04d09f4252a2fee17ae70b8bc0"}},"metadata":{}},{"name":"stdout","text":"Q: How is PCOS diagnosed and treated?\nRouted to: ['medical_qa', 'women_health']\nA: The diagnosis of PCOS is based on the presence of at least two out of three criteria, which includes: 1) clinical or biochemical evidence of high androgens (such as acne, hirsutism, or male-pattern hair loss), 2) irregular menstruation (oligo / anovulatory), and 3) polycystic ovaries on ultrasound or histology (which means the presence of multiple small cysts on the ovaries). In addition to these criteria, other laboratory tests may be performed to rule out other conditions that can mimic the symptoms of PCOS, such as thyroid disorders or adrenal gland disorders. [Source 5 from medical _ qa]: The diagnosis of PCOS typically involves a combination of clinical evaluation, physical examination, and blood tests to measure hormone levels. The management of PCOS primarily focuses on addressing the underlying hormonal imbalances and managing associated symptoms. This may include lifestyle changes (such as diet and exercise), medications (like metformin for insulin resistance or birth control pills to regulate periods), and potentially fertility treatments if you desire pregnancy\nMetrics: retrieval=4.957 completeness=0.235 faithfulness=1.000 composite=2.353 \n--------------------------------------------------------------------------------\n\n[Query 5/5]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c26f26f16054000b4f6ecc6c6e0e0f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6307cb3cb39547ff9289aa12bbc6da6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74aff5b3c5a14a6ea4848ae431caaf03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0701c7fae48949b682b3fdece74487cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3640dc158ceb4d30a6f8672b091d6624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e380c61adabf497c9b469084f4646062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5476263d2446409d8232d39f1bd29b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4688ee51dcb840309f43bb1241eee215"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab984337f45645548b2f8b6f27a67a77"}},"metadata":{}},{"name":"stdout","text":"Q: What are the differences between Type 1 and Type 2 diabetes?\nRouted to: ['medical_qa', 'women_health']\nA: Both types of diabetes have similar symptoms, but they differ in etiology, complications, and treatment options. [Source 5 from medical _ qa]: Diabetes is a serious illness that affects multiple organ systems. It is characterized by hyperglycemia resulting from defects in insulin production or action. Type 1 diabetes is more common than type 2 diabetes. Type 1 diabetes is caused by autoimmune destruction of pancreatic beta cells. Type 2 diabetes is caused by insulin resistance and impaired insulin secretion. [Source 6 from medical _ qa]: diabetes is a serious illness that affects multiple organ systems. It is characterized by hyperglycemia resulting from defects in insulin production or action. Type 1 diabetes is more common than type 2 diabetes. Type 1 diabetes is caused by autoimmune destruction of pancreatic beta cells. Type 2 diabetes is caused by insulin resistance and impaired insulin secretion. [Source 7 from medical _ qa]: diabetes is a serious illness that affects multiple organ systems. It is characterized by hyperglycemia resulting from defects in insulin production or action. Type\nMetrics: retrieval=6.776 completeness=0.167 faithfulness=0.929 composite=3.039 \n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5f3df0383b47a080f93863d98c7ffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38322722c1d349f9befb0434a46eb802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4eb75c1e8c741cfafb5338417378db9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a273333fc24d47822b642921a54ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"535572b497a54f7fb2ae49f7211c31e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1266253b02464f27809e6105f6a28699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39a677c2323432da53bd1134f3f1c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd20690dad242db98fd685d4d7afe46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d0c2affcf74ee4959417504ab009a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ce25136bd6453fa0b776fbf6b2d9e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a425857b334c37b425dd9556e904f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30a4c09180e40d8967c9c2adf80af71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac4538476374b1bb12e5152a8b31397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56006678e4754a14b7206afeb66c6a15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e823ae6d59e244fa9e6f37f6a5abbbab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbf3b105b9964ac4b2e94a2c958b8497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d37069df3f74ed28ff68c4d8709cea6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16dc474261b2406298dee4f7cb4b2b37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41bcdbfca3d246d6abcfa3ba7bf4948a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ac8fe6db094ae4a8b6872ec9d07d68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3dcdb20b3e9472aa32aa0270bfe9aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89e4907d6f6447ed949ed842f50b55e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccaa71a2ee754924bd32257b98e61ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ee71bbf08c4cf58f8bae6b774728b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a7923b9ed24a79a7b56152368d3517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02ae66edefde45e5ad8e89598da6649a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc1c575d87e4069bb0416f2c647841f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7b07f739dc4750bde38c207f1ac35a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb372c976b31430bb396e94319fc6e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"655ccdca44dd4c5f89c170825c99cec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1493a7c6f6944da0b41c8e57c992def1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff62890977f44cf48bcf51c4f6bbe36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"989a6e42e2c94be4a90e0418c1c1d724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c276e5f9614f84bf53472187cff830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dfb5080a0f247fb94ba9d75e5495367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68d789fe14942dcb7108129b4d60f1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27933edd4dba4531918c721346f0a703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c60098754d4f1a8585021efcc31ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fde0c1ecfb548a5ba18066f13d7ae93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c118cac1b4471c9c11a328ea06c41d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29cf89c63ced4d5e9e9224e436ceecb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80bfaa8c98ad41939ef3375b725cb64c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e725cc65d7406caf9379f3a2752109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26482b98c0b447bd9b0a9aa13dcafa3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90aef8fac489495e85986d776f91d542"}},"metadata":{}},{"name":"stdout","text":"\nüìà BATCH EVALUATION SUMMARY\n================================================================================\nSuccess Rate: 100.0%\nAverage Retrieval: 3.744\nAverage Completeness: 0.264\nAverage Faithfulness: 0.806\nAverage Composite: 1.818\n================================================================================\n\nüìù Prompt logs saved to prompts_outputs.pkl\n\n‚úÖ MULTI-DOMAIN RAG PIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}